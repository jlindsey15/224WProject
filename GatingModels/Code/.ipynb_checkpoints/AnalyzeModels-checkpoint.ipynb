{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/druckmannuser/dual_alm_analysis/GatingModels/Code\n",
      "../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC4/BAYLORGC4_2018_03_22.mat\n",
      "BRIDGE no\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './GatingModels/SavedPreds/OneStepModel1.0_attractor_normalbridge_0.001BAYLORGC4_2018_03_22.mat_TRAIN_BASELINE_L.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4684fc448589>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mtrain_baseline_L\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb_file\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'TRAIN_BASELINE_L.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0mtrain_baseline_R\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb_file\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'TRAIN_BASELINE_R.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mtest_baseline_L\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb_file\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'TEST_BASELINE_L.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_pathlib_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './GatingModels/SavedPreds/OneStepModel1.0_attractor_normalbridge_0.001BAYLORGC4_2018_03_22.mat_TRAIN_BASELINE_L.npy'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import time\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import pickle\n",
    "\n",
    "from lfads_datasets import DualALMDataset\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "print(os.getcwd())\n",
    "upcwd = '..'\n",
    "stimtypecwd = upcwd + '/stimtypes'\n",
    "logcwd = upcwd + '/Analyses'\n",
    "cwd = upcwd + '/SavedPreds'\n",
    "filenames = ['../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC4/BAYLORGC4_2018_03_22.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC4/BAYLORGC4_2018_03_21.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC4/BAYLORGC4_2018_03_23.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC4/BAYLORGC4_2018_03_20.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC4/BAYLORGC4_2018_03_25.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC12/BAYLORGC12_2018_04_03.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC12/BAYLORGC12_2018_04_04.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC12/BAYLORGC12_2018_04_05.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC12/BAYLORGC12_2018_04_06.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC13/BAYLORGC13_2018_04_23.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC13/BAYLORGC13_2018_04_24.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC13/BAYLORGC13_2018_04_25.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC15/BAYLORGC15_2018_05_07.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC15/BAYLORGC15_2018_05_08.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC15/BAYLORGC15_2018_05_09.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC15/BAYLORGC15_2018_05_10.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC15/BAYLORGC15_2018_05_11.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC17/BAYLORGC17_2018_06_08.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC17/BAYLORGC17_2018_06_11.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC18/BAYLORGC18_2018_05_31.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC19/BAYLORGC19_2018_06_20.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC19/BAYLORGC19_2018_06_21.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC19/BAYLORGC19_2018_06_22.mat']\n",
    "\n",
    "savenames = ['BAYLORGC4_2018_03_22.mat', 'BAYLORGC4_2018_03_21.mat', 'BAYLORGC4_2018_03_23.mat', 'BAYLORGC4_2018_03_20.mat', 'BAYLORGC4_2018_03_25.mat', 'BAYLORGC12_2018_04_03.mat', 'BAYLORGC12_2018_04_04.mat', 'BAYLORGC12_2018_04_05.mat', 'BAYLORGC12_2018_04_06.mat', 'BAYLORGC13_2018_04_23.mat', 'BAYLORGC13_2018_04_24.mat', 'BAYLORGC13_2018_04_25.mat', 'BAYLORGC15_2018_05_07.mat', 'BAYLORGC15_2018_05_08.mat', 'BAYLORGC15_2018_05_09.mat', 'BAYLORGC15_2018_05_10.mat', 'BAYLORGC15_2018_05_11.mat', 'BAYLORGC17_2018_06_08.mat', 'BAYLORGC17_2018_06_11.mat', 'BAYLORGC18_2018_05_31.mat', 'BAYLORGC19_2018_06_20.mat', 'BAYLORGC19_2018_06_21.mat', 'BAYLORGC19_2018_06_22.mat']\n",
    "\n",
    "trainset = False\n",
    "\n",
    "\n",
    "for timebins in ['']:\n",
    "  for trainstimtype in ['']:\n",
    "    variance_explained = {}\n",
    "\n",
    "\n",
    "    gate_types = ['no', '', 'TanhSquare_scalargateFromAll', 'FlipTanhSquare_scalargateFromAll', \n",
    "                  'MonoTanhSquare_scalargateFromAll','attractor_TanhSquare_scalargateFromAll', \n",
    "                  'attractor_FlipTanhSquare_scalargateFromAll', 'attractor_MonoTanhSquare_scalargateFromAll',\n",
    "                 'attractor_nobridge', 'attractor_normalbridge']\n",
    "    \n",
    "    totvariance_explained = {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for bridge in gate_types:\n",
    "        variance_explained[bridge] = []\n",
    "        totvariance_explained[bridge] = []\n",
    "        for indexX in range(len(filenames)):\n",
    "          filename = filenames[indexX]\n",
    "          savename = savenames[indexX]\n",
    "          print(filename)\n",
    "          tempvars = {0.001: [], 0.0001: [], 0.00001: [], 0.000001: []}\n",
    "          tempvarsnum = {0.001: [], 0.0001: [], 0.00001: [], 0.000001: []}\n",
    "          tempvarsdenom = {0.001: [], 0.0001: [], 0.00001: [], 0.000001: []}\n",
    "          for wd in [0.001, 0.0001, 0.00001, 0.000001]:\n",
    "            weight_decay = wd\n",
    "\n",
    "\n",
    "            no_f = 'nof'\n",
    "            print('BRIDGE', bridge)\n",
    "            if bridge == '' or bridge == 'no':\n",
    "                pred_file = 'X'+bridge + 'bridgeFullToCDPred'\n",
    "            else:\n",
    "                pred_file = 'OneStepModel' + str(1.0) + '_' + str(bridge) + '_' + str(timebins)+str(trainstimtype)+str(weight_decay)+ savename + '_'\n",
    "\n",
    "\n",
    "            b_file = 'OneStepModel' + str(1.0) + '_' + str('attractor_normalbridge') + '_' + str(timebins)+str(trainstimtype)+str(weight_decay)+ savename + '_'#'XRegSuperSimple' + str(1.0) + '_' + '_' + str(timebins)+str(trainstimtype)+ savename + '_'\n",
    "\n",
    "\n",
    "            train_baseline_L = np.load(cwd+'/'+b_file+'TRAIN_BASELINE_L.npy')\n",
    "            train_baseline_R = np.load(cwd+'/'+b_file+'TRAIN_BASELINE_R.npy')\n",
    "            test_baseline_L = np.load(cwd+'/'+b_file+'TEST_BASELINE_L.npy')\n",
    "            test_baseline_R = np.load(cwd+'/'+b_file+'TEST_BASELINE_R.npy')\n",
    "            if trainset:\n",
    "              test_baseline_L = train_baseline_L\n",
    "              test_baseline_R = train_baseline_R\n",
    "                \n",
    "            stimtype_file = 'XRegSuperSimple' + str(1.0) + '_' + '_' + str(timebins)+str(trainstimtype)+ savename + '_'\n",
    "            train_stimtypes = np.load(stimtypecwd+'/'+stimtype_file + 'TRAIN_stimtypes.npy')\n",
    "            #print('stst', train_stimtypes.shape, train_baseline_L.shape, train_baseline_R.shape)\n",
    "            test_stimtypes = np.load(stimtypecwd+'/'+stimtype_file + 'TEST_stimtypes.npy')\n",
    "            if trainset:\n",
    "                test_stimtypes = train_stimtypes\n",
    "\n",
    "\n",
    "            if bridge == '' or bridge == 'no':\n",
    "                model_preds_train_L = np.squeeze(np.load(cwd+'/'+pred_file+'TRAINL'+timebins+trainstimtype+'.npy')[indexX])#[:, :, :]#[:, 13:, :]\n",
    "                model_preds_train_R = np.squeeze(np.load(cwd+'/'+pred_file+'TRAINR'+timebins+trainstimtype+'.npy')[indexX])#[:, :, :]#[:, 13:, :]\n",
    "            else:\n",
    "                model_preds_train_L = np.load(cwd+'/'+pred_file+'TRAIN_L.npy')#[:, :, :]#[:, 13:, :]\n",
    "                model_preds_train_R = np.load(cwd+'/'+pred_file+'TRAIN_R.npy')#[:, :, :]#[:, 13:, :]\n",
    "            if trainset:\n",
    "              model_preds_test_L = model_preds_train_L\n",
    "              model_preds_test_R = model_preds_train_R\n",
    "            else: \n",
    "                if bridge == '' or bridge == 'no':\n",
    "                    model_preds_test_L = np.squeeze(np.load(cwd+'/'+pred_file+'TESTL'+timebins+trainstimtype+'.npy')[indexX])#[:, :, :]#[:, 13:, :]#model_preds_train_L#np.load(args.pred_file+'TEST_L.npy')#[:, :, :]#[:, 13:, :]\n",
    "                    model_preds_test_R = np.squeeze(np.load(cwd+'/'+pred_file+'TESTR'+timebins+trainstimtype+'.npy')[indexX])#[:, :, :]#[:, 13:, :]#model_preds_train_R#np.load(args.pred_file+'TEST_R.npy')#[:, :, :]#[:, 13:, :]\n",
    "\n",
    "                else:\n",
    "                    model_preds_test_L = np.load(cwd+'/'+pred_file+'TEST_L.npy')#[:, :, :]#[:, 13:, :]\n",
    "                    model_preds_test_R = np.load(cwd+'/'+pred_file+'TEST_R.npy')#[:, :, :]#[:, 13:, :]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "            if model_preds_train_L.shape[0] < 0:\n",
    "                continue\n",
    "\n",
    "\n",
    "\n",
    "            train_data_L = np.load(cwd+'/'+b_file+'TRAIN_TRUTH_L.npy')# - train_baseline_L\n",
    "            train_data_R = np.load(cwd+'/'+b_file+'TRAIN_TRUTH_R.npy')# - train_baseline_R\n",
    "\n",
    "            if trainset:\n",
    "                test_data_L = train_data_L\n",
    "                test_data_R = train_data_R\n",
    "            else:\n",
    "                test_data_L = np.load(cwd+'/'+b_file+'TEST_TRUTH_L.npy')# - test_baseline_L\n",
    "                test_data_R = np.load(cwd+'/'+b_file+'TEST_TRUTH_R.npy')# - test_baseline_R\n",
    "\n",
    "\n",
    "\n",
    "            print(model_preds_train_L.shape, model_preds_train_R.shape, model_preds_test_L.shape, model_preds_test_R.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            model_preds_trainX = [model_preds_train_L, model_preds_train_R]\n",
    "            model_preds_testX = [model_preds_test_L, model_preds_test_R]\n",
    "            train_dataX = [train_data_L, train_data_R]\n",
    "            test_dataX = [test_data_L, test_data_R]\n",
    "            train_baselineX = [train_baseline_L, train_baseline_R]\n",
    "            test_baselineX = [test_baseline_L, test_baseline_R]\n",
    "            locs = ['left_ALM', 'right_ALM']\n",
    "            for leftright in [0, 1]:\n",
    "              for stype in [[0,0], [1,2], [2,1], [3,3]]:\n",
    "                if leftright == 0:\n",
    "                    print('LEFTTTTTT')\n",
    "                else:\n",
    "                    print('RIGHTTTTT')\n",
    "                \n",
    " \n",
    "                numneurons = 1#model_preds_trainX[leftright].shape[1]\n",
    "                if timebins == '':\n",
    "                    model_preds_train = model_preds_trainX[leftright].reshape(8, -1, numneurons)\n",
    "                    model_preds_test = model_preds_testX[leftright].reshape(8, -1, numneurons)\n",
    "                    train_baseline = train_baselineX[leftright].reshape(8, -1, numneurons)\n",
    "                    test_baseline = test_baselineX[leftright].reshape(8, -1, numneurons)\n",
    "                    train_data = train_dataX[leftright].reshape(8, -1, numneurons)\n",
    "                    test_data = test_dataX[leftright].reshape(8, -1, numneurons)\n",
    "                else:\n",
    "                    model_preds_train = model_preds_trainX[leftright].reshape(4, -1, numneurons)\n",
    "                    model_preds_test = model_preds_testX[leftright].reshape(4, -1, numneurons)\n",
    "                    train_baseline = train_baselineX[leftright].reshape(4, -1, numneurons)\n",
    "                    test_baseline = test_baselineX[leftright].reshape(4, -1, numneurons)\n",
    "                    train_data = train_dataX[leftright].reshape(4, -1, numneurons)\n",
    "                    test_data = test_dataX[leftright].reshape(4, -1, numneurons)\n",
    "\n",
    "                train_data = train_data[:, train_stimtypes == stype[leftright]].reshape(-1, numneurons)\n",
    "                model_preds_train = model_preds_train[:, train_stimtypes == stype[leftright]].reshape(-1, numneurons)\n",
    "                train_baseline = train_baseline[:, train_stimtypes == stype[leftright]].reshape(-1, numneurons)\n",
    "                test_data = test_data[:, test_stimtypes == stype[leftright]].reshape(-1, numneurons)\n",
    "                model_preds_test = model_preds_test[:, test_stimtypes == stype[leftright]].reshape(-1, numneurons)\n",
    "\n",
    "                test_baseline = test_baseline[:, test_stimtypes == stype[leftright]].reshape(-1, numneurons)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "                test_data_pred = model_preds_test\n",
    "                train_data_pred = model_preds_train\n",
    "\n",
    "     \n",
    "                avg_se_per_trial = np.sum(np.square(test_data_pred - test_data))/model_preds_test.shape[0]\n",
    "\n",
    "                \n",
    "                total_avg_pred = test_baseline#np.mean(train_data, axis=0, keepdims=True) * 10\n",
    "                total_avg_pred_se_per_trial = np.sum(np.square(total_avg_pred - test_data))/model_preds_test.shape[0]\n",
    "\n",
    "\n",
    "                print('Variance Explained', 1 - avg_se_per_trial/total_avg_pred_se_per_trial)\n",
    "\n",
    "                tempvars[wd].append(1 - avg_se_per_trial/total_avg_pred_se_per_trial)\n",
    "                tempvarsnum[wd].append(avg_se_per_trial*test_baseline.shape[0])\n",
    "                tempvarsdenom[wd].append(total_avg_pred_se_per_trial*test_baseline.shape[0])\n",
    "          maxval = -999999999999999\n",
    "          maxkey = 200\n",
    "          for key in tempvars.keys():\n",
    "              val = 1 - (np.sum(tempvarsnum[key]) / np.sum(tempvarsdenom[key])) \n",
    "              if val > maxval:\n",
    "                  maxval = val\n",
    "                  maxkey = key\n",
    "          variance_explained[bridge].extend(tempvars[maxkey])\n",
    "          totvariance_explained[bridge].append(maxval)\n",
    "         \n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "    name = logcwd+'/'+'OneStepModels_var_explained'+ str(timebins)+str(trainstimtype)+'.npy'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "    if trainset:\n",
    "        name = 'TRAIN' + name\n",
    "\n",
    "    np.save(name, variance_explained)\n",
    "\n",
    "    name = logcwd+'/'+'OneStepModels_totvar_explained'+ str(timebins)+str(trainstimtype)+'.npy'\n",
    "   \n",
    "    if trainset:\n",
    "        name = 'TRAIN' + name\n",
    "\n",
    "\n",
    "    np.save(name, totvariance_explained)\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no 0.1369457667132536\n",
      " 0.1440035813260241\n",
      "TanhSquare_scalargateFromAll 0.1546237545123432\n",
      "FlipTanhSquare_scalargateFromAll 0.15381489056525055\n",
      "MonoTanhSquare_scalargateFromAll 0.15220510307854546\n",
      "attractor_TanhSquare_scalargateFromAll 0.16080449767701135\n",
      "attractor_FlipTanhSquare_scalargateFromAll 0.15846336813879014\n",
      "attractor_MonoTanhSquare_scalargateFromAll 0.1512604884723392\n",
      "attractor_nobridge 0.13766135306033128\n",
      "attractor_normalbridge 0.14502153071584567\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ttest_relResult(statistic=1.6434840720387838, pvalue=0.11449949128130214)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "totvar = np.load(logcwd+'/'+'OneStepModels_totvar_explained'+'.npy').tolist()\n",
    "for bridge in gate_types:\n",
    "    print(bridge, np.mean(totvar[bridge]))\n",
    "    \n",
    "ttest_rel(totvar['attractor_TanhSquare_scalargateFromAll'], totvar['TanhSquare_scalargateFromAll'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no 0.14602075856426758 0.13816171094536736 0.11598594682816676 0.14974842727560445\n",
      " 0.1640665521718036 0.13845808958618136 0.11176430188560722 0.16526974172740017\n",
      "TanhSquare_scalargateFromAll 0.16254209298646702 0.16398823439083374 0.12284785200843863 0.17201545549389144\n",
      "FlipTanhSquare_scalargateFromAll 0.17078419737395215 0.15089411032511224 0.12793524802055975 0.16801222973660362\n",
      "MonoTanhSquare_scalargateFromAll 0.16158653898449601 0.1604292147076677 0.12648077872806016 0.16194763525704045\n",
      "attractor_TanhSquare_scalargateFromAll 0.17161450372148326 0.16582736457224717 0.13945259775192922 0.16742733005946064\n",
      "attractor_FlipTanhSquare_scalargateFromAll 0.16982337334636513 0.16390305159686244 0.13096972379072927 0.17129611495768643\n",
      "attractor_MonoTanhSquare_scalargateFromAll 0.16109990136457922 0.16332673137730838 0.12260131345215602 0.159364711539908\n",
      "attractor_nobridge 0.14623296365135022 0.13900975299954413 0.11735376636915407 0.15012644445346585\n",
      "attractor_normalbridge 0.1579288361800759 0.1436360595133214 0.11319988279638489 0.16938130710515145\n"
     ]
    }
   ],
   "source": [
    "var = np.load(logcwd+'/'+'OneStepModels_totvar_explained'+'.npy').tolist()\n",
    "for bridge in gate_types:\n",
    "    vals = var[bridge]\n",
    "\n",
    "\n",
    "\n",
    "    vals_n = vals[::4]\n",
    "    vals_i = vals[1::4]\n",
    "    vals_c = vals[2::4]\n",
    "    vals_b = vals[3::4]\n",
    "    print(bridge, np.mean(vals_n), np.mean(vals_i), np.mean(vals_c), np.mean(vals_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
