{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.378\n",
      "0.06961441922106126 0.03746760534720084\n",
      "0.005\n",
      "0.05348454172887296 0.025167252136048646\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-375299fd2eda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0mregr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRidgeCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m             \u001b[0mregr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_fulllabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_fulllabels\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mregr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/linear_model/ridge.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1112\u001b[0m                                   \u001b[0mgcv_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcv_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m                                   store_cv_values=self.store_cv_values)\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_cv_values\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/linear_model/ridge.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                 \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQT_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                 \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQT_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/linear_model/ridge.py\u001b[0m in \u001b[0;36m_errors_svd\u001b[0;34m(self, alpha, y, v, U, UT_y)\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_errors_svd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUT_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m         \u001b[0mG_diag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_errors_and_values_svd_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUT_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mG_diag\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/linear_model/ridge.py\u001b[0m in \u001b[0;36m_errors_and_values_svd_helper\u001b[0;34m(self, alpha, y, v, U, UT_y)\u001b[0m\n\u001b[1;32m    953\u001b[0m         \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconstant_column\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0;31m# cancel the regularization for the intercept\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_diag_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUT_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m         \u001b[0mG_diag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decomp_diag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# One-step ahead linear regression.  Does regression from full activity to full activity, \n",
    "# full activity to CD, and CD to CD. This code is currently only considering the late delay period (post-perturbation).\n",
    "\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "from scipy.stats import ttest_rel\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, MultiTaskLassoCV\n",
    "import pickle\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,\"\n",
    "\n",
    "from lfads_datasets import DualALMSimpleDataset\n",
    "\n",
    "cwd = '..'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "filenames = ['../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC4/BAYLORGC4_2018_03_22.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC4/BAYLORGC4_2018_03_21.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC4/BAYLORGC4_2018_03_23.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC4/BAYLORGC4_2018_03_20.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC4/BAYLORGC4_2018_03_25.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC12/BAYLORGC12_2018_04_03.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC12/BAYLORGC12_2018_04_04.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC12/BAYLORGC12_2018_04_05.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC12/BAYLORGC12_2018_04_06.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC13/BAYLORGC13_2018_04_23.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC13/BAYLORGC13_2018_04_24.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC13/BAYLORGC13_2018_04_25.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC15/BAYLORGC15_2018_05_07.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC15/BAYLORGC15_2018_05_08.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC15/BAYLORGC15_2018_05_09.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC15/BAYLORGC15_2018_05_10.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC15/BAYLORGC15_2018_05_11.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC17/BAYLORGC17_2018_06_08.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC17/BAYLORGC17_2018_06_11.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC18/BAYLORGC18_2018_05_31.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC19/BAYLORGC19_2018_06_20.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC19/BAYLORGC19_2018_06_21.mat', '../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC19/BAYLORGC19_2018_06_22.mat']\n",
    "\n",
    "savenames = ['BAYLORGC4_2018_03_22.mat', 'BAYLORGC4_2018_03_21.mat', 'BAYLORGC4_2018_03_23.mat', 'BAYLORGC4_2018_03_20.mat', 'BAYLORGC4_2018_03_25.mat', 'BAYLORGC12_2018_04_03.mat', 'BAYLORGC12_2018_04_04.mat', 'BAYLORGC12_2018_04_05.mat', 'BAYLORGC12_2018_04_06.mat', 'BAYLORGC13_2018_04_23.mat', 'BAYLORGC13_2018_04_24.mat', 'BAYLORGC13_2018_04_25.mat', 'BAYLORGC15_2018_05_07.mat', 'BAYLORGC15_2018_05_08.mat', 'BAYLORGC15_2018_05_09.mat', 'BAYLORGC15_2018_05_10.mat', 'BAYLORGC15_2018_05_11.mat', 'BAYLORGC17_2018_06_08.mat', 'BAYLORGC17_2018_06_11.mat', 'BAYLORGC18_2018_05_31.mat', 'BAYLORGC19_2018_06_20.mat', 'BAYLORGC19_2018_06_21.mat', 'BAYLORGC19_2018_06_22.mat']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for timebins in ['']: #'first' and 'last' let you do regression only on the first/second half of the late delay period\n",
    "    for trainstimtype in ['']: #choose 'uni', 'early delay bi ALM', or 'no stim' for unilateral pert, bilateral pert, or control trials\n",
    "\n",
    "        #Nomenclature: \"nobridge\" means independent predictions on each hemisphere.  \"bridge\" means using activity\n",
    "        #from both hemisphere to make predictions.  \"XToY\" means using X to make predictions about Y -- so for example\n",
    "        # \"FullTCD\" means predicting CD activity at time t+1 from full activity at time t\n",
    "        alpha_valsFullToCD = []\n",
    "        alpha_valsCDToCD = []\n",
    "        alpha_valsFullToFull = []\n",
    "        ratio_valsFullToCD = []\n",
    "        ratio_valsCDToCD = []\n",
    "        ratio_valsFullToFull = []\n",
    "        \n",
    "        bridgeFullToCD = []\n",
    "        nobridgeFullToCD = []\n",
    "        bridgeCDToCD = []\n",
    "        nobridgeCDToCD = []\n",
    "        bridgeFullToFull = []\n",
    "        nobridgeFullToFull = []\n",
    "        bridgeFullToCDPredTRAINL = []\n",
    "        bridgeCDToCDPredTRAINL = []\n",
    "        bridgeFullToFullPredTRAINL = []\n",
    "        bridgeFullToCDPredTESTL = []\n",
    "        bridgeCDToCDPredTESTL = []\n",
    "        bridgeFullToFullPredTESTL = []\n",
    "\n",
    "        bridgeFullToCDPredTRAINR = []\n",
    "        bridgeCDToCDPredTRAINR = []\n",
    "        bridgeFullToFullPredTRAINR = []\n",
    "        bridgeFullToCDPredTESTR = []\n",
    "        bridgeCDToCDPredTESTR = []\n",
    "        bridgeFullToFullPredTESTR = []\n",
    "\n",
    "        nobridgeFullToCDPredTRAINL = []\n",
    "        nobridgeFullToCDPredTRAINR = []\n",
    "        nobridgeCDToCDPredTRAINL = []\n",
    "        nobridgeCDToCDPredTRAINR = []\n",
    "        nobridgeFullToFullPredTRAINL = []\n",
    "        nobridgeFullToFullPredTRAINR = []\n",
    "        nobridgeFullToCDPredTESTL = []\n",
    "        nobridgeFullToCDPredTESTR = []\n",
    "        nobridgeCDToCDPredTESTL = []\n",
    "        nobridgeCDToCDPredTESTR = []\n",
    "        nobridgeFullToFullPredTESTL = []\n",
    "        nobridgeFullToFullPredTESTR = []\n",
    "        \n",
    "        \n",
    "        for indexX in range(len(filenames)):\n",
    "            filename = filenames[indexX]\n",
    "            savename = savenames[indexX]\n",
    "            save_preds = ''\n",
    "            if timebins == '':\n",
    "                bins = range(8)\n",
    "            elif timebins == 'firstbins':\n",
    "                bins = range(4)\n",
    "            elif timebins == 'lastbins':\n",
    "                bins = range(4, 8)\n",
    "\n",
    "\n",
    "            #loads the information about which trials are which stim type\n",
    "            #only important if you have chosen to do the regression on a subset of trials, selected by stim type\n",
    "            pred_file = 'OneStepModel' + str(1.0) + '_' + str('attractor_normalbridge') + '_' + str(timebins)+str(trainstimtype)+str(0.001)+savename + '_'\n",
    "            stimtype_file = 'XRegSuperSimple' + str(1.0) + '_' + '_' + str(timebins)+str(trainstimtype)+ savename + '_'\n",
    "\n",
    "            train_stimtypes = np.load(cwd+'/stimtypes/'+stimtype_file + 'TRAIN_stimtypes.npy')\n",
    "            test_stimtypes = np.load(cwd+'/stimtypes/'+stimtype_file + 'TEST_stimtypes.npy')\n",
    "\n",
    "            if trainstimtype == '':\n",
    "                trainstimfilter = train_stimtypes < 100\n",
    "            elif trainstimtype == 'uni':\n",
    "                trainstimfilter = (train_stimtypes % 3) != 0 \n",
    "            elif trainstimtype == 'early delay bi ALM':\n",
    "                trainstimfilter = train_stimtypes == 3\n",
    "            elif trainstimtype == 'no stim':\n",
    "                trainstimfilter = train_stimtypes == 0\n",
    "\n",
    "            if trainstimtype == '':\n",
    "                teststimfilter = test_stimtypes < 100\n",
    "            elif trainstimtype == 'uni':\n",
    "                teststimfilter = (test_stimtypes % 3) != 0 \n",
    "            elif trainstimtype == 'early delay bi ALM':\n",
    "                teststimfilter = test_stimtypes == 3\n",
    "            elif trainstimtype == 'no stim':\n",
    "                teststimfilter = test_stimtypes == 0\n",
    "                \n",
    "            #loads and preprocesses the data.  This takes some time so I have saved the preprocessed version for\n",
    "            #fast loading.  However if you want to mess around with the data preprocessing, choice of time bins, etc.\n",
    "            # you will want to edit the DualALMSimpleDataset class and uncomment + rerun these lines\n",
    "            \n",
    "            #train_dataset = DualALMSimpleDataset(train=True, neuron_split=1.0, location=None, filename=filename, train_stim_type=trainstimtype, timebins=timebins)\n",
    "            #test_dataset = DualALMSimpleDataset(train=False, neuron_split=1.0, location=None, filename=filename, train_stim_type=trainstimtype, timebins=timebins)\n",
    "\n",
    "            #filehandler = open(args.filename+'ALLEVERYTHING_train_dataset.obj', 'wb')\n",
    "            #pickle.dump(train_dataset, filehandler)\n",
    "            #filehandler = open(args.filename+'ALLEVERYTHING_test_dataset.obj', 'wb')\n",
    "            #pickle.dump(test_dataset, filehandler\n",
    "            filehandler = open(cwd+'/OneStepPreprocessed/'+savename+'ALLEVERYTHING_train_dataset.obj', 'rb')\n",
    "            train_dataset = pickle.load(filehandler)\n",
    "            filehandler = open(cwd+'/OneStepPreprocessed/'+savename+'ALLEVERYTHING_test_dataset.obj', 'rb')\n",
    "            test_dataset = pickle.load(filehandler)\n",
    "\n",
    "            train_data = train_dataset.data.reshape(8, -1, train_dataset.data.shape[1])[bins][:, trainstimfilter].reshape(-1, train_dataset.data.shape[1])\n",
    "            train_CDdata = train_dataset.CDdata.reshape(8, -1, 2)[bins][:, trainstimfilter].reshape(-1, 2)\n",
    "            train_data_L = train_data[:, train_dataset.pa.train_data.neural_unit_location=='left_ALM']\n",
    "            train_data_R = train_data[:, train_dataset.pa.train_data.neural_unit_location=='right_ALM']\n",
    "            train_data_CDL = train_CDdata[:, 0:1]\n",
    "            train_data_CDR = train_CDdata[:, 1:2]\n",
    "            train_labels = train_dataset.labels.reshape(8, -1, 2)[bins][:, trainstimfilter].reshape(-1, 2)\n",
    "            train_fulllabels = train_dataset.fulllabels.reshape(8, -1, train_dataset.data.shape[1])[bins][:, trainstimfilter].reshape(-1, train_dataset.data.shape[1])\n",
    "            train_labels_L = train_labels[:, 0]\n",
    "            train_labels_R = train_labels[:, 1]\n",
    "            train_labels_fullL = train_fulllabels[:, train_dataset.pa.train_data.neural_unit_location=='left_ALM']\n",
    "            train_labels_fullR = train_fulllabels[:, train_dataset.pa.train_data.neural_unit_location=='right_ALM']\n",
    "            test_data = test_dataset.data.reshape(8, -1, test_dataset.data.shape[1])[bins][:, teststimfilter].reshape(-1, test_dataset.data.shape[1])\n",
    "            test_CDdata = test_dataset.CDdata.reshape(8, -1, 2)[bins][:, teststimfilter].reshape(-1, 2)\n",
    "            test_data_L = test_data[:, train_dataset.pa.train_data.neural_unit_location=='left_ALM']\n",
    "            test_data_R = test_data[:, train_dataset.pa.train_data.neural_unit_location=='right_ALM']\n",
    "            test_data_CDL = test_CDdata[:, 0:1]\n",
    "            test_data_CDR = test_CDdata[:, 1:2]\n",
    "            test_labels = test_dataset.labels.reshape(8, -1, 2)[bins][:, teststimfilter].reshape(-1, 2)\n",
    "            test_fulllabels = test_dataset.fulllabels.reshape(8, -1, test_dataset.data.shape[1])[bins][:, teststimfilter].reshape(-1, test_dataset.data.shape[1])\n",
    "            test_labels_L = test_labels[:, 0]\n",
    "            test_labels_R = test_labels[:, 1]\n",
    "            test_labels_fullL = test_fulllabels[:, train_dataset.pa.train_data.neural_unit_location=='left_ALM']\n",
    "            test_labels_fullR = test_fulllabels[:, train_dataset.pa.train_data.neural_unit_location=='right_ALM']\n",
    "\n",
    "            \n",
    "            #Do the regressions.  the \"alphas\" parameter gives the range of L2 regularization factors to try\n",
    "            regr = RidgeCV(alphas=np.arange(0.001, 1.0, 0.001))\n",
    "            regr.fit(train_data, train_labels)\n",
    "            loss = np.sum(np.square(test_labels - regr.predict(test_data)))\n",
    "            bridgeFullToCDPredTRAINL.append(regr.predict(train_data)[:, 0:1])\n",
    "            bridgeFullToCDPredTESTL.append(regr.predict(test_data)[:, 0:1])\n",
    "\n",
    "            bridgeFullToCDPredTRAINR.append(regr.predict(train_data)[:, 1:2])\n",
    "            bridgeFullToCDPredTESTR.append(regr.predict(test_data)[:, 1:2])\n",
    "            alpha_valsFullToCD.append(regr.alpha_)\n",
    "            #ratio_valsFullToCD.append(regr.l1_ratio)\n",
    "            print(regr.alpha_)\n",
    "\n",
    "            regr = RidgeCV(alphas=np.arange(0.001, 1.0, 0.001))\n",
    "            regr.fit(train_data_L, train_labels_L)\n",
    "            loss_L = np.sum(np.square(test_labels_L - regr.predict(test_data_L)))\n",
    "            train_pred_L = regr.predict(train_data_L)\n",
    "            test_pred_L = regr.predict(test_data_L)\n",
    "            regr = RidgeCV(alphas=np.arange(0.001, 1.0, 0.001))\n",
    "            regr.fit(train_data_R, train_labels_R)\n",
    "            loss_R = np.sum(np.square(test_labels_R - regr.predict(test_data_R)))\n",
    "            train_pred_R = regr.predict(train_data_R)\n",
    "            test_pred_R = regr.predict(test_data_R)\n",
    "            nobridgeFullToCDPredTRAINL.append(train_pred_L)\n",
    "            nobridgeFullToCDPredTESTL.append(test_pred_L)\n",
    "            nobridgeFullToCDPredTRAINR.append(train_pred_R)\n",
    "            nobridgeFullToCDPredTESTR.append(test_pred_R)\n",
    "\n",
    "            #the \"baseline\" for variance explained figures is just the identity transformation\n",
    "            baseline = np.sum(np.square(test_labels - test_CDdata))\n",
    "            print(1 - loss/baseline, 1 - (loss_L + loss_R)/baseline)\n",
    "            #print(regr.alpha_)\n",
    "\n",
    "            bridgeFullToCD.append(1 - loss/baseline)\n",
    "            nobridgeFullToCD.append(1 - (loss_L + loss_R)/baseline)\n",
    "\n",
    "\n",
    "            regr = RidgeCV(alphas=np.arange(0.001, 1.0, 0.001))\n",
    "            regr.fit(train_CDdata, train_labels)\n",
    "            loss = np.sum(np.square(test_labels - regr.predict(test_CDdata)))\n",
    "\n",
    "            bridgeCDToCDPredTRAINL.append(regr.predict(train_CDdata)[:, 0:1])\n",
    "            bridgeCDToCDPredTESTL.append(regr.predict(test_CDdata)[:, 0:1])\n",
    "\n",
    "            bridgeCDToCDPredTRAINR.append(regr.predict(train_CDdata)[:, 1:2])\n",
    "            bridgeCDToCDPredTESTR.append(regr.predict(test_CDdata)[:, 1:2])\n",
    "            alpha_valsCDToCD.append(regr.alpha_)\n",
    "            #ratio_valsCDToCD.append(regr.l1_ratio)\n",
    "            print(regr.alpha_)\n",
    "\n",
    "            regr = RidgeCV(alphas=np.arange(0.001, 1.0, 0.001))\n",
    "            regr.fit(train_data_CDL, train_labels_L)\n",
    "            loss_L = np.sum(np.square(test_labels_L - regr.predict(test_data_CDL)))\n",
    "            train_pred_L = regr.predict(train_data_CDL)\n",
    "            test_pred_L = regr.predict(test_data_CDL)\n",
    "            regr = RidgeCV(alphas=np.arange(0.001, 1.0, 0.001))\n",
    "            regr.fit(train_data_CDR, train_labels_R)\n",
    "            loss_R = np.sum(np.square(test_labels_R - regr.predict(test_data_CDR)))\n",
    "            train_pred_R = regr.predict(train_data_CDR)\n",
    "            test_pred_R = regr.predict(test_data_CDR)\n",
    "            nobridgeCDToCDPredTRAINL.append(train_pred_L)\n",
    "            nobridgeCDToCDPredTESTL.append(test_pred_L)\n",
    "            nobridgeCDToCDPredTRAINR.append(train_pred_R)\n",
    "            nobridgeCDToCDPredTESTR.append(test_pred_R)\n",
    "            baseline = np.sum(np.square(test_labels - test_CDdata))\n",
    "            print(1 - loss/baseline, 1 - (loss_L + loss_R)/baseline)\n",
    "            #print(regr.alpha_)\n",
    "\n",
    "            bridgeCDToCD.append(1 - loss/baseline)\n",
    "            nobridgeCDToCD.append(1 - (loss_L + loss_R)/baseline)\n",
    "\n",
    "            regr = RidgeCV(alphas=np.arange(0.001, 1.0, 0.001))\n",
    "            regr.fit(train_data, train_fulllabels)\n",
    "            loss = np.sum(np.square(test_fulllabels - regr.predict(test_data)))\n",
    "\n",
    "            bridgeFullToFullPredTRAINL.append(regr.predict(train_data)[:, train_dataset.pa.train_data.neural_unit_location=='left_ALM'])\n",
    "            bridgeFullToFullPredTESTL.append(regr.predict(test_data)[:, train_dataset.pa.train_data.neural_unit_location=='left_ALM'])\n",
    "\n",
    "            bridgeFullToFullPredTRAINR.append(regr.predict(train_data)[:, train_dataset.pa.train_data.neural_unit_location=='right_ALM'])\n",
    "            bridgeFullToFullPredTESTR.append(regr.predict(test_data)[:, train_dataset.pa.train_data.neural_unit_location=='right_ALM'])\n",
    "\n",
    "            alpha_valsFullToFull.append(regr.alpha_)\n",
    "            #ratio_valsFullToFull.append(regr.l1_ratio)\n",
    "            print(regr.alpha_)\n",
    "\n",
    "            regr = RidgeCV(alphas=np.arange(0.001, 1.0, 0.001))\n",
    "            regr.fit(train_data_L, train_labels_fullL)\n",
    "            loss_L = np.sum(np.square(test_labels_fullL - regr.predict(test_data_L)))\n",
    "            train_pred_L = regr.predict(train_data_L)\n",
    "            test_pred_L = regr.predict(test_data_L)\n",
    "            regr = RidgeCV(alphas=np.arange(0.001, 1.0, 0.001))\n",
    "            regr.fit(train_data_R, train_labels_fullR)\n",
    "            loss_R = np.sum(np.square(test_labels_fullR - regr.predict(test_data_R)))\n",
    "            train_pred_R = regr.predict(train_data_R)\n",
    "            test_pred_R = regr.predict(test_data_R)\n",
    "            nobridgeFullToFullPredTRAINL.append(train_pred_L)\n",
    "            nobridgeFullToFullPredTESTL.append(test_pred_L)\n",
    "            nobridgeFullToFullPredTRAINR.append(train_pred_R)\n",
    "            nobridgeFullToFullPredTESTR.append(test_pred_R)\n",
    "            baseline = np.sum(np.square(test_fulllabels - test_data))\n",
    "            print(1 - loss/baseline, 1 - (loss_L + loss_R)/baseline)\n",
    "            #print(regr.alpha_)\n",
    "\n",
    "            bridgeFullToFull.append(1 - loss/baseline)\n",
    "            nobridgeFullToFull.append(1 - (loss_L + loss_R)/baseline)\n",
    "        \n",
    "        #saves the variance explained for each session, full activity to full activity regression\n",
    "        np.save('XbridgeFullToFull'+timebins+trainstimtype, np.squeeze(bridgeFullToFull))\n",
    "        \n",
    "        #saves the full-to-full regresssion predictions for each session on the train/test set and left/right hemispheres\n",
    "        np.save(cwd+'/SavedPreds/'+'XbridgeFullToFullPredTRAINL'+timebins+trainstimtype, np.squeeze(bridgeFullToFullPredTRAINL))\n",
    "        np.save(cwd+'/SavedPreds/'+'XbridgeFullToFullPredTESTL'+timebins+trainstimtype, np.squeeze(bridgeFullToFullPredTESTL))\n",
    "        np.save(cwd+'/SavedPreds/'+'XbridgeFullToFullPredTRAINR'+timebins+trainstimtype, np.squeeze(bridgeFullToFullPredTRAINR))\n",
    "        np.save(cwd+'/SavedPreds/'+'XbridgeFullToFullPredTESTR'+timebins+trainstimtype, np.squeeze(bridgeFullToFullPredTESTR))\n",
    "        \n",
    "        np.save(cwd+'/SavedPreds/'+'XnobridgeFullToFull'+timebins+trainstimtype, np.squeeze(nobridgeFullToFull))\n",
    "        np.save(cwd+'/SavedPreds/'+'XnobridgeFullToFullPredTRAINL'+timebins+trainstimtype, np.squeeze(nobridgeFullToFullPredTRAINL))\n",
    "        np.save(cwd+'/SavedPreds/'+'XnobridgeFullToFullPredTESTL'+timebins+trainstimtype, np.squeeze(nobridgeFullToFullPredTESTL))\n",
    "        np.save(cwd+'/SavedPreds/'+'XnobridgeFullToFullPredTRAINR'+timebins+trainstimtype, np.squeeze(nobridgeFullToFullPredTRAINR))\n",
    "        np.save(cwd+'/SavedPreds/'+'XnobridgeFullToFullPredTESTR'+timebins+trainstimtype, np.squeeze(nobridgeFullToFullPredTESTR))\n",
    "        \n",
    "        np.save(cwd+'/SavedPreds/'+'XbridgeFullToCD'+timebins+trainstimtype, np.squeeze(bridgeFullToCD))\n",
    "        np.save(cwd+'/SavedPreds/'+'XbridgeFullToCDPredTRAINL'+timebins+trainstimtype, np.squeeze(bridgeFullToCDPredTRAINL))\n",
    "        np.save(cwd+'/SavedPreds/'+'XbridgeFullToCDPredTESTL'+timebins+trainstimtype, np.squeeze(bridgeFullToCDPredTESTL))\n",
    "        np.save(cwd+'/SavedPreds/'+'XbridgeFullToCDPredTRAINR'+timebins+trainstimtype, np.squeeze(bridgeFullToCDPredTRAINR))\n",
    "        np.save(cwd+'/SavedPreds/'+'XbridgeFullToCDPredTESTR'+timebins+trainstimtype, np.squeeze(bridgeFullToCDPredTESTR))\n",
    "        \n",
    "        np.save(cwd+'/SavedPreds/'+'XnobridgeFullToCD'+timebins+trainstimtype, np.squeeze(nobridgeFullToCD))\n",
    "        np.save(cwd+'/SavedPreds/'+'XnobridgeFullToCDPredTRAINL'+timebins+trainstimtype, np.squeeze(nobridgeFullToCDPredTRAINL))\n",
    "        np.save(cwd+'/SavedPreds/'+'XnobridgeFullToCDPredTESTL'+timebins+trainstimtype, np.squeeze(nobridgeFullToCDPredTESTL))\n",
    "        np.save(cwd+'/SavedPreds/'+'XnobridgeFullToCDPredTRAINR'+timebins+trainstimtype, np.squeeze(nobridgeFullToCDPredTRAINR))\n",
    "        np.save(cwd+'/SavedPreds/'+'XnobridgeFullToCDPredTESTR'+timebins+trainstimtype, np.squeeze(nobridgeFullToCDPredTESTR))\n",
    "        \n",
    "        np.save(cwd+'/SavedPreds/'+'XbridgeCDToCD'+timebins+trainstimtype, np.squeeze(bridgeCDToCD))\n",
    "        np.save(cwd+'/SavedPreds/'+'XbridgeCDToCDPredTRAINL'+timebins+trainstimtype, np.squeeze(bridgeCDToCDPredTRAINL))\n",
    "        np.save(cwd+'/SavedPreds/'+'XbridgeCDToCDPredTESTL'+timebins+trainstimtype, np.squeeze(bridgeCDToCDPredTESTL))\n",
    "        np.save(cwd+'/SavedPreds/'+'XbridgeCDToCDPredTRAINR'+timebins+trainstimtype, np.squeeze(bridgeCDToCDPredTRAINR))\n",
    "        np.save(cwd+'/SavedPreds/'+'XbridgeCDToCDPredTESTR'+timebins+trainstimtype, np.squeeze(bridgeCDToCDPredTESTR))\n",
    "        \n",
    "        np.save(cwd+'/SavedPreds/'+'XnobridgeCDToCDPredTRAINL'+timebins+trainstimtype, np.squeeze(nobridgeCDToCDPredTRAINL))\n",
    "        np.save(cwd+'/SavedPreds/'+'XnobridgeCDToCDPredTESTL'+timebins+trainstimtype, np.squeeze(nobridgeCDToCDPredTESTL))\n",
    "        np.save(cwd+'/SavedPreds/'+'XnobridgeCDToCDPredTRAINR'+timebins+trainstimtype, np.squeeze(nobridgeCDToCDPredTRAINR))\n",
    "        np.save(cwd+'/SavedPreds/'+'XnobridgeCDToCDPredTESTR'+timebins+trainstimtype, np.squeeze(nobridgeCDToCDPredTESTR))\n",
    "        np.save(cwd+'/SavedPreds/'+'XnobridgeCDToCD'+timebins+trainstimtype, np.squeeze(nobridgeCDToCD))\n",
    "        \n",
    "        #saves the optimal L2 regularization parameters for each session and each regression type\n",
    "        np.save(cwd+'/SavedPreds/'+'XalphavalsFullToFull'+timebins+trainstimtype, alpha_valsFullToFull)\n",
    "        np.save(cwd+'/SavedPreds/'+'XalphavalsFullToCD'+timebins+trainstimtype, alpha_valsFullToCD)\n",
    "        np.save(cwd+'/SavedPreds/'+'XalphavalsCDToCD'+timebins+trainstimtype, alpha_valsCDToCD)\n",
    "        \n",
    "\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "os.getcwd()\n",
    "os.chdir('../../Code/Python')\n",
    "sys.path.append('')\n",
    "\n",
    "from neupop import Session as Session\n",
    "from neupop import PopulationAnalysis as PA\n",
    "from neupop import MultiSessionAnalysis as MS\n",
    "from neupop import decoders as ld\n",
    "from neupop import session_parsers as parsers\n",
    "\n",
    "class DualALMSimpleDataset(data.Dataset):\n",
    "    def __init__(self, train, neuron_split=1.0, back_half_neurons=False, normalize=True, location='left_ALM', train_stim_type='no stim', test_stim_type=None, filename='../../Data/DualALMRecordinbgsDataSet2018_04_14/BAYLORGC12/BAYLORGC12_2018_04_04.mat', only_correct=True, timebins='', indexX = 0):\n",
    "        # Determine normalizing factor, which is the maximum value across both train and test set.\n",
    "\n",
    "        sd = Session(filename, parser=parsers.ParserNuoLiDualALM)\n",
    "        self.train = train\n",
    "        pa = PA(sd)\n",
    "\n",
    "        (start, end) = pa.score_input_trial_range()\n",
    "        pa.cull_data(start, end)\n",
    "        if train:\n",
    "            pa.load_train_and_test(train_stim_type=train_stim_type, test_stim_type=None, train_proportion=0.8, location=location)\n",
    "        else:\n",
    "            pa.load_train_and_test(train_stim_type=train_stim_type, test_stim_type=None, train_proportion=0.8, location=location)\n",
    "\n",
    "        if only_correct:\n",
    "            correct_mask = pa.input_data.behavior_report == 1\n",
    "            self.fraction_correct = np.sum(pa.train_data.behavior_report) / len(pa.train_data.behavior_report)\n",
    "            #print(filename, train_stim_type, location, train)\n",
    "            #print('Frac correct', self.fraction_correct)\n",
    "            self.behavior_report = pa.train_data.behavior_report\n",
    "            if True:#train_stim_type != 'early delay bi ALM':\n",
    "                pa.filter_train_trials_from_input(correct_mask)\n",
    "                pa.filter_test_trials_from_input(correct_mask)\n",
    "\n",
    "        pa.preprocess(bin_width=0.4, label_by_report=True, even_test_classes=False)\n",
    "\n",
    "        train_data = pa.train_rates\n",
    "        test_data = pa.test_rates\n",
    "\n",
    "        if train:\n",
    "            self.norm_factor = np.amax(train_data)\n",
    "        else:\n",
    "            self.norm_factor = max(np.amax(train_data), np.amax(test_data))\n",
    "        self.train_label = pa.train_label\n",
    "        self.test_label = pa.test_label\n",
    "\n",
    "        if not normalize:\n",
    "            self.norm_factor = 1\n",
    "\n",
    "\n",
    "        self.pa = pa\n",
    "        if True:#train_stim_type == 'no stim':\n",
    "            self.coding_directions = [pa.compute_CD_for_each_bin()[0][i].W for i in range(len(pa.compute_CD_for_each_bin()[0]))]\n",
    "\n",
    "        if train:\n",
    "            self.data = train_data / self.norm_factor\n",
    "            self.task_trial_type= pa.train_data.task_trial_type\n",
    "            self.pole_on_time = np.mean(pa.train_data.task_pole_on_time)\n",
    "            self.pole_off_time = np.mean(pa.train_data.task_pole_off_time)\n",
    "            self.cue_on_time = np.mean(pa.train_data.task_cue_on_time)\n",
    "            self.behavior_report_type = pa.train_data.behavior_report_type\n",
    "\n",
    "  \n",
    " \n",
    "        else:\n",
    "            self.data = test_data / self.norm_factor\n",
    "            self.task_trial_type= pa.test_data.task_trial_type\n",
    "            self.pole_on_time = np.mean(pa.test_data.task_pole_on_time)\n",
    "            self.pole_off_time = np.mean(pa.test_data.task_pole_off_time)\n",
    "            self.cue_on_time = np.mean(pa.test_data.task_cue_on_time)\n",
    "            self.behavior_report_type = pa.test_data.behavior_report_type\n",
    "\n",
    "        #self.data = self.data.astype(float)\n",
    "        #print('data shape', self.data.shape)\n",
    "        if back_half_neurons:\n",
    "            self.data = self.data[:, :, (int)(neuron_split*self.data.shape[2]):]\n",
    "        else:\n",
    "            self.data = self.data[:, :, :(int)(neuron_split*self.data.shape[2])]\n",
    "        if timebins == '':\n",
    "            times = [28, 29, 30, 31, 32, 33, 34, 35]\n",
    "        elif timebins == 'firstbins':\n",
    "            times = [28, 29, 30, 31]\n",
    "        elif timebins == 'lastbins':\n",
    "            times = [32, 33, 34, 35]\n",
    "        labeltimes = [time + 1 for time in times]\n",
    "        num_neurons = self.data.shape[2]\n",
    "        datatemp = self.data\n",
    "        self.data = datatemp[times, :, :].reshape(-1, num_neurons) #data is full activityat time t\n",
    "        #print('data shape', self.data.shape)\n",
    "        self.labels = datatemp[labeltimes, :, :].reshape(-1, num_neurons) #temp, to be changed later in the code\n",
    "\n",
    "        self.cd_L = np.expand_dims(self.coding_directions[36][pa.train_data.neural_unit_location=='left_ALM'], 1)\n",
    "        self.cd_R = np.expand_dims(self.coding_directions[36][pa.train_data.neural_unit_location=='right_ALM'], 1)\n",
    "        self.cd_L /= np.linalg.norm(self.cd_L)\n",
    "        self.cd_R /= np.linalg.norm(self.cd_R)\n",
    "        self.fulllabels = self.labels #fulllabels are full activity at time t+1\n",
    "        labelsL = np.matmul(self.labels[:, pa.train_data.neural_unit_location=='left_ALM'], self.cd_L)\n",
    "        labelsR = np.matmul(self.labels[:, pa.train_data.neural_unit_location=='right_ALM'], self.cd_R)\n",
    "        dataL = np.matmul(self.data[:, pa.train_data.neural_unit_location=='left_ALM'], self.cd_L)\n",
    "        dataR = np.matmul(self.data[:, pa.train_data.neural_unit_location=='right_ALM'], self.cd_R)\n",
    "        self.CDdata = np.concatenate([dataL, dataR], 1) #CDdata is CD activity at time t\n",
    "        self.labels = np.concatenate([labelsL, labelsR], 1) # labels are CD activity at time t+1\n",
    "\n",
    "        linreg_file = 'bridgeFullToCDPred'\n",
    "        \n",
    "        if train:\n",
    "            self.linreg_L = np.squeeze(np.load(linreg_file+'TRAINL'+'.npy')[indexX])#[:, :, :]#[:, 13:, :]\n",
    "            self.linreg_R = np.squeeze(np.load(linreg_file+'TRAINR'+'.npy')[indexX])#[:, :, :]#[:, 13:, :]\n",
    "        else:\n",
    "            self.linreg_L = np.squeeze(np.load(linreg_file+'TESTL'+'.npy')[indexX])#[:, :, :]#[:, 13:, :]\n",
    "            self.linreg_R = np.squeeze(np.load(linreg_file+'TESTR'+'.npy')[indexX])#[:, :, :]#[:, 13:, :]\n",
    "\n",
    "        linreg_file = 'nobridgeFullToCDPred'\n",
    "        \n",
    "        if train:\n",
    "            self.nobridge_linreg_L = np.squeeze(np.load(linreg_file+'TRAINL'+'.npy')[indexX])#[:, :, :]#[:, 13:, :]\n",
    "            self.nobridge_linreg_R = np.squeeze(np.load(linreg_file+'TRAINR'+'.npy')[indexX])#[:, :, :]#[:, 13:, :]\n",
    "        else:\n",
    "            self.nobridge_linreg_L = np.squeeze(np.load(linreg_file+'TESTL'+'.npy')[indexX])#[:, :, :]#[:, 13:, :]\n",
    "            self.nobridge_linreg_R = np.squeeze(np.load(linreg_file+'TESTR'+'.npy')[indexX])#[:, :, :]#[:, 13:, :]\n",
    "        \n",
    "\n",
    "        #assert self.data.shape[1] == self.labels.shape[1]\n",
    "\n",
    "    def get_data_max(self):\n",
    "        return np.amax(self.data)\n",
    "\n",
    "    def get_data_min(self):\n",
    "        return np.amin(self.data)\n",
    "\n",
    "    def get_norm_factor(self):\n",
    "        return self.norm_factor\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Note that for sequence datasets like this, data has shape (seq_len, n_data, input_size).\n",
    "        #print('My index is ', index)\n",
    "        x = self.data[index,:]\n",
    "        y = self.labels[index, :]\n",
    "        linreg_L = self.linreg_L[index]\n",
    "        linreg_R = self.linreg_R[index]\n",
    "        nobridge_linreg_L = self.nobridge_linreg_L[index]\n",
    "        nobridge_linreg_R = self.nobridge_linreg_R[index]\n",
    "        return x, y, linreg_L, linreg_R, nobridge_linreg_L, nobridge_linreg_R\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        # Note that for sequence datasets like this, data has shape (seq_len, n_data, input_size).\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def get_len(self):\n",
    "        # Note that for sequence datasets like this, data has shape (seq_len, n_data, input_size).\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
